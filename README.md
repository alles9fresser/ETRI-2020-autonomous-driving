# MobileNet-V1


Simple pytorch jupyter notebook implementations of ["MobileNet V1"] (https://arxiv.org/abs/1704.04861)

Posts on : https://alles9fresser.blogspot.com/2020/08/mobilenet-v1.html


<h3 style="text-align: left;"><span style="font-family: inherit;"><span style="font-weight: normal;"><span>이 글에서는&nbsp;</span></span><span style="font-weight: normal;">대표적인&nbsp;Lightweight architecture</span> <span style="font-weight: normal;">인</span>&nbsp;<i style="font-weight: normal;">MobileNet (v1)</i><span style="font-weight: normal;">과 <i>ShuffleNet (v1)</i></span><i style="font-weight: normal;">&nbsp;</i><span style="font-weight: normal;">에 대해서 다뤄보겠다.</span></span></h3><h3 style="text-align: left;"><span style="font-weight: normal;"><span style="font-family: inherit;"><br /></span></span></h3><h1 style="text-align: left;"><span><span style="font-family: inherit;">MobileNet <i>v1</i></span></span></h1><h3 style="text-align: left;"><span style="font-family: inherit;"><span><br /></span><span style="font-weight: normal;"><i>MobileNet</i>은 Standard Convolution Layer -&gt; Depth wise Separable Convolution Layer로 바꾸는게 핵심이므로 이 부분을 주로 다루겠다.</span></span></h3><div><span style="font-weight: normal;"><span style="font-family: inherit;"><br /></span></span></div><h3><span><span style="font-family: inherit; font-size: x-large;"><i>1.Standard Convolution ( Full Convolution )&nbsp;</i></span></span></h3><div><span><span style="font-family: inherit; font-size: x-large;"><i><br /></i></span></span></div><h3 style="text-align: left;"><span style="font-family: inherit;"><span style="font-weight: normal;"></span><span style="font-size: medium; font-weight: normal;">Standard Convolution Layer구성을:&nbsp;</span></span></h3><h3 style="text-align: left;"><span style="font-weight: normal;"><span style="font-family: inherit;">- \(F/G\) -&gt; Input 체널 2, Output 체널 3개,&nbsp;&nbsp;</span></span></h3><h3 style="text-align: left;"><span style="font-weight: normal;"><span style="font-family: inherit;">- Height/Width -&gt; \( D_F \)로 i/o 크기동일, Square input image</span></span></h3><h3 style="text-align: left;"><span style="font-family: inherit;"><span style="font-weight: normal;">-</span><span style="font-weight: normal;">&nbsp;Kernel&nbsp; \(K\): \(D_K \times D_K\) , stride=1, with padding이라 하면,</span></span></h3><div><span style="font-family: inherit; font-weight: normal;"><br /></span></div><h3 style="text-align: left;"><span style="font-family: inherit;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<img border="0" data-original-height="720" data-original-width="1280" height="281" src="https://1.bp.blogspot.com/-RqlW02wJtOM/XzFuFnqzLII/AAAAAAAAABU/PExwZP2ie-EinVbgspyT183TngQtENUEACNcBGAsYHQ/w500-h281/%25EC%258A%25AC%25EB%259D%25BC%25EC%259D%25B4%25EB%2593%259C3.BMP" style="font-weight: normal;" width="500" /></span></h3><h3 style="text-align: left;"><span><span style="font-family: inherit; font-weight: 400;">수식으로 나타내보면,</span></span></h3><h3 style="text-align: left;"><span style="font-family: inherit; font-weight: normal;"><span>$$ G_{k,l,n} = \sum_{i,j,m} K_{i,j,m,n} F_{k+i-1, l+j-1, m} $$</span><span><br /></span><span>computation complexity는 \( D_K * D_K * D_F * D_f&nbsp; \) 를 2x3 번 하니까 $$ D_K * D_K * D_F * D_F * 2 * 3 $$ 가 된다.</span></span></h3><h3 style="text-align: left;"><span style="font-family: inherit;"><span style="font-weight: normal;"><br /></span><span><i style="font-size: x-large;">2.Depthwise Separable Convolution</i></span></span></h3><div><span style="font-family: inherit;"><br /></span></div><div><span><span style="font-family: inherit;"><h3 style="text-align: left;"><span><span style="font-size: medium;"><span style="font-weight: normal;">Depth wise separable convolution은 </span><i>Depthwise conv. -&gt; Pointwise conv.</i><span style="font-weight: normal;"> 2 stage로 구성되어 있다, 1.의 Standard conv. 과 동일한 setting이라 가정한다면:</span></span></span></h3><div><span style="font-weight: normal;"><span style="font-size: medium;"><br /></span></span></div></span></span></div><h3 style="text-align: left;"><span style="font-family: inherit;"><span style="font-weight: normal;"></span><span style="font-size: x-large;"><i>2-1.Depthwise convolution</i></span></span></h3><h3 style="text-align: left;"><span style="font-family: inherit;">&nbsp; &nbsp; &nbsp;<a href="https://1.bp.blogspot.com/-QQqV1rDlBSY/XzFuFro6PFI/AAAAAAAAABk/uO4sghjN-cAGmkMq3VogQzrAP9B3rQefACPcBGAYYCw/s1280/%25EC%258A%25AC%25EB%259D%25BC%25EC%259D%25B4%25EB%2593%259C1.BMP" style="font-weight: normal; margin-left: 1em; margin-right: 1em; text-align: center;"><img border="0" data-original-height="720" data-original-width="1280" height="288" src="https://1.bp.blogspot.com/-QQqV1rDlBSY/XzFuFro6PFI/AAAAAAAAABk/uO4sghjN-cAGmkMq3VogQzrAP9B3rQefACPcBGAYYCw/w512-h288/%25EC%258A%25AC%25EB%259D%25BC%25EC%259D%25B4%25EB%2593%259C1.BMP" width="512" /></a>&nbsp;</span></h3><h3 style="text-align: left;"><span style="font-family: inherit; font-weight: normal;"><span>Depth wise convolution은 모든 2x3 체널 조합에 대해 convolution을 진행하지 않고&nbsp;</span><span>group size= input channel size로 나눠서 진행한다.&nbsp;</span></span></h3><h3 style="text-align: left;"><span style="font-family: inherit;"><span style="font-weight: normal;"><span>group size = input channel size 니까 output channel 크기 = input channel 크기가 되겠고 convolution은 각 group 내부에서만 하니까 complexity는:&nbsp;</span></span>$$D_K*D_K*2*D_F*D_F$$</span></h3><h3 style="text-align: left;"><span style="font-family: inherit;"><span style="font-weight: normal;"><br /></span><span style="font-size: x-large;"><i>2-2. Pointwise convolution ( 1x1 conv. )</i></span></span></h3><h3 style="text-align: left;"><span style="font-family: inherit;">&nbsp; &nbsp; &nbsp;<a href="https://1.bp.blogspot.com/-U5xlQsK9dzg/XzFuFZPWEXI/AAAAAAAAABc/Vvih1Sx1hAAsEG9zmwadS8pXNeJir6joQCPcBGAYYCw/s1280/%25EC%258A%25AC%25EB%259D%25BC%25EC%259D%25B4%25EB%2593%259C2.BMP" style="margin-left: 1em; margin-right: 1em; text-align: center;"><img border="0" data-original-height="720" data-original-width="1280" height="288" src="https://1.bp.blogspot.com/-U5xlQsK9dzg/XzFuFZPWEXI/AAAAAAAAABc/Vvih1Sx1hAAsEG9zmwadS8pXNeJir6joQCPcBGAYYCw/w512-h288/%25EC%258A%25AC%25EB%259D%25BC%25EC%259D%25B4%25EB%2593%259C2.BMP" width="512" /></a>&nbsp;</span></h3><h3><span style="font-family: inherit;"><span><span style="font-weight: normal;">Depth wise convolution의 output을&nbsp;</span><span><span style="font-weight: normal;">standard 1x1 convolution (point wise convolution을 해주어 그룹/channel 간의 feature를 합쳐준다. \(D_K = 1\),&nbsp;</span></span></span><span style="font-weight: normal;">Complexity는</span></span></h3><h3><span style="font-family: inherit;">$$ D_K*D_K*2*3*D_F*D_F $$</span></h3><h3><span style="font-weight: normal;"><span style="font-family: inherit;"><br /></span></span></h3><h3><span style="font-family: inherit; font-weight: normal;"><span>2 stage의 complexity를&nbsp;</span><span>전부 더한 total complexity는 :&nbsp;<br /></span><span>$$ D_K*D_K*2*D_F*D_F&nbsp; + 1*1*2*D_F*D_F $$&nbsp;</span><span><br /></span><span>이제 kernel size=\(D_K\) , i/o channel size=\( m, n \) 으로 두 complexity의 ratio를 구해보면:<br /></span><span>$$ \frac{ D_K*D_K*m*D_F*D_F&nbsp; + 1*1*m*n*D_F*D_F }{ D_K * D_K * D_F * D_F * m* n } = \frac{1}{n} + \frac{1}{D_K^2}$$</span><span><br /></span><span><i>MobileNet </i>논문에서는 \(D_K = 3\) 으로 한다니까 계산량이 8x ~ 9x 줄어들게 된다.&nbsp;</span><span><br /></span><span>결국 stage 1 에서는 체널당 한번씩만 Kernel 로 feature를 추출 하고 stage 2 에서 1x1 full convolution (point wise convolution) 으로 feature들을 합처주는 역할을 하게 되는 것 같다.&nbsp;</span></span></h3><h3><span style="font-weight: normal;"><span style="font-family: inherit;">이렇게 되면 당연히 standard full convolution layer 보다 expressibility가 줄어들겠지만 accuracy는 별로 많이 줄어들지는 않는다는게 <span style="font-style: italic;">MobileNet v1 </span>의 아이디어다.&nbsp;</span></span></h3><div><span style="font-weight: normal;"><span style="font-family: inherit;"><br /></span></span></div><div><span style="font-weight: normal;"><span><h3><span style="font-family: inherit; font-weight: normal;"><span>여기서 point wise convolution이 하는 feature를 합치는 역할을 shuffle layer로 대체하게 되는게&nbsp;<i>ShuffleNet&nbsp;</i>이다.</span><span><br /></span></span></h3><div><span style="font-family: inherit; font-weight: normal;"><span><br /></span></span></div><div><span style="font-family: inherit; font-weight: normal;"><span><br /></span></span></div></span></span></div><h1 style="text-align: left;"><span><span style="font-family: inherit;"><b>ShuffleNet <i>v1</i></b></span></span></h1><div><span><span style="font-family: inherit;"><b><i><br /></i></b></span></span></div><h3 style="text-align: left;"><span style="font-weight: normal;"><span style="font-family: inherit;">MobileNets의 구조를 보면 depthwise convolution (# group = # input channel ) 으로 같은 group 내부에서 feature extraction을 한 후 pointwise convolution (1x1 standard convolution) 을 통해 서로 다른 group의 output feature 를 섞어준다.&nbsp;</span></span></h3><div><span style="font-weight: normal;"><span style="font-family: inherit;"><br /></span></span></div><div><span style="font-weight: normal;"><span style="font-family: inherit;"><h3><a href="https://1.bp.blogspot.com/-mwgFbq-gKGo/XzQjSjo77BI/AAAAAAAAAB8/XtrzRRQ0RYEd6PGJCf_sL_gJccdXkyByQCNcBGAsYHQ/s1463/shuffle2.JPG" style="font-weight: 400; margin-left: 1em; margin-right: 1em; text-align: center;"><img border="0" data-original-height="681" data-original-width="1463" src="https://1.bp.blogspot.com/-mwgFbq-gKGo/XzQjSjo77BI/AAAAAAAAAB8/XtrzRRQ0RYEd6PGJCf_sL_gJccdXkyByQCNcBGAsYHQ/s640/shuffle2.JPG" width="640" /></a></h3></span></span></div><h3 style="text-align: left;"><span style="font-weight: normal;"><h3 style="text-align: left;"><span style="font-family: inherit; font-weight: normal;"><br /></span></h3></span></h3><h3><span style="font-weight: normal;"><span style="font-family: inherit;">ShuffleNet 논문에 나오는 내용에 따르면 pointwise convolution이 MobileNets의 계산량의 대부분을 차지하게 되는데 ( kernel size가 1x1 이라고 해도 full convolution 은 input channel X output channel 전부에 대해서 해줘야 하기 때문일듯)&nbsp;ShuffleNet은 channel shuffle operation + group conv를 통해 서로 다른 group 간의 feature 를 "shuffle" 한다.</span></span></h3><div><span style="font-weight: normal;"><span style="font-family: inherit;"><br /></span></span></div><div><span style="font-weight: normal;"><span style="font-family: inherit;"><img border="0" data-original-height="651" data-original-width="1463" src="https://1.bp.blogspot.com/-rPRyeGmpB7o/XzQidPnDhSI/AAAAAAAAABw/VSCWerz7sr4FkeyL_xympN9A1irAaouJACNcBGAsYHQ/s640/shuffle1.JPG" width="640" /></span></span></div><h3><span style="font-weight: normal;"><span style="font-family: inherit;"><br /></span></span></h3><h3><span style="font-weight: normal;"><span style="font-family: inherit;">Shuffle operation의 실제 구현은 channel dimension을 group x channel size (per group)로 shape를 변환한 후 2d transpose를 하게 된다.</span></span></h3><div><span style="font-weight: normal;"><span style="font-family: inherit;"><br /></span></span></div><h3><span style="font-weight: normal;"><span style="font-family: inherit;">결국 Shuffle operation과 함께 group conv에서 group size 파라미터를 조절해서 channel 간 connection sparsity를 더욱 flexible하게 조절할 수 있게 되는 것이다.&nbsp;</span></span></h3><h3><span style="font-family: inherit;"><span style="font-weight: normal;"><span>i.e group 의 갯수를 늘리면 sparse 해지고 group의 갯수를 줄이면 group의 크기가 커지므로 더욱 많은 connection이 생긴다.&nbsp;</span></span><span style="font-weight: normal;">Complexity를 산출하는 과정은</span> <i>MobileNet</i><span style="font-weight: normal;">이랑 비슷하게 진행된다(논문을 참고하자).</span></span></h3>
